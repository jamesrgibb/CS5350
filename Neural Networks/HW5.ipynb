{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T21:13:21.987836Z",
     "start_time": "2023-12-10T21:13:21.980646Z"
    }
   },
   "id": "d3ef310ac33d6e18"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 2A: Backpropagation\n",
      "Backpropagation Gradients Example:\n",
      "Weight Gradients: [array([[-3.51210427, -9.26731517,  3.51940575,  3.85408744],\n",
      "       [-3.62988658, -9.57810486,  3.63743292,  3.98333854],\n",
      "       [-0.53829502, -1.42038766,  0.5394141 ,  0.59071027],\n",
      "       [-1.73313934, -4.5731981 ,  1.73674245,  1.90189985],\n",
      "       [-0.83426319, -2.20135262,  0.83599758,  0.91549767],\n",
      "       [ 2.49257621,  6.57710806, -2.49775814, -2.73528515],\n",
      "       [ 0.77190929,  2.03682071, -0.77351405, -0.84707221],\n",
      "       [ 0.05224832,  0.13786655, -0.05235695, -0.05733589],\n",
      "       [-0.08384749, -0.2212466 ,  0.08402181,  0.09201195],\n",
      "       [ 3.30475842,  8.72019608, -3.31162884, -3.62655177]]), array([[ 5.01193811e-04,  8.02345365e-08,  1.62653771e+00,\n",
      "         1.62643781e+00,  1.60079468e+00,  6.97652709e-05,\n",
      "         1.61181089e+00,  1.62590287e+00,  1.62653771e+00,\n",
      "         8.62127730e-02],\n",
      "       [-3.73820470e-04, -5.98437400e-08, -1.21316959e+00,\n",
      "        -1.21309508e+00, -1.19396889e+00, -5.20351325e-05,\n",
      "        -1.20218545e+00, -1.21269609e+00, -1.21316959e+00,\n",
      "        -6.43026682e-02],\n",
      "       [ 5.74879037e-05,  9.20305720e-09,  1.86567035e-01,\n",
      "         1.86555576e-01,  1.83614259e-01,  8.00221210e-06,\n",
      "         1.84877841e-01,  1.86494218e-01,  1.86567035e-01,\n",
      "         9.88877254e-03],\n",
      "       [ 5.29908764e-05,  8.48314228e-09,  1.71972711e-01,\n",
      "         1.71962149e-01,  1.69250919e-01,  7.37623405e-06,\n",
      "         1.70415656e-01,  1.71905590e-01,  1.71972711e-01,\n",
      "         9.11521711e-03],\n",
      "       [ 2.14590660e-04,  3.43531420e-08,  6.96416822e-01,\n",
      "         6.96374050e-01,  6.85394711e-01,  2.98706314e-05,\n",
      "         6.90111403e-01,  6.96145010e-01,  6.96416821e-01,\n",
      "         3.69127780e-02],\n",
      "       [ 9.60087947e-05,  1.53697451e-08,  3.11579915e-01,\n",
      "         3.11560778e-01,  3.06648574e-01,  1.33642504e-05,\n",
      "         3.08758843e-01,  3.11458305e-01,  3.11579914e-01,\n",
      "         1.65149374e-02],\n",
      "       [-3.81387781e-05, -6.10551671e-09, -1.23772799e-01,\n",
      "        -1.23765197e-01, -1.21813861e-01, -5.30884886e-06,\n",
      "        -1.22652149e-01, -1.23724490e-01, -1.23772799e-01,\n",
      "        -6.56043580e-03],\n",
      "       [ 2.17485065e-04,  3.48164982e-08,  7.05810111e-01,\n",
      "         7.05766762e-01,  6.94639334e-01,  3.02735273e-05,\n",
      "         6.99419645e-01,  7.05534633e-01,  7.05810110e-01,\n",
      "         3.74106586e-02],\n",
      "       [ 5.40486635e-05,  8.65248009e-09,  1.75405576e-01,\n",
      "         1.75394803e-01,  1.72629452e-01,  7.52347610e-06,\n",
      "         1.73817439e-01,  1.75337116e-01,  1.75405576e-01,\n",
      "         9.29717219e-03],\n",
      "       [ 3.36769388e-05,  5.39123493e-09,  1.09292672e-01,\n",
      "         1.09285960e-01,  1.07562909e-01,  4.68776890e-06,\n",
      "         1.08303127e-01,  1.09250015e-01,  1.09292672e-01,\n",
      "         5.79293323e-03]]), array([[2.33610681, 2.53447748, 0.01401273, 1.93062949, 1.46127511,\n",
      "        1.85275909, 2.5549448 , 0.01402147, 1.6113085 , 0.03756783]])]\n",
      "Bias Gradients: [array([[-0.91268529],\n",
      "       [-0.9432932 ],\n",
      "       [-0.13988592],\n",
      "       [-0.45038833],\n",
      "       [-0.21679873],\n",
      "       [ 0.64774206],\n",
      "       [ 0.20059492],\n",
      "       [ 0.01357769],\n",
      "       [-0.02178932],\n",
      "       [ 0.85880264]]), array([[ 1.62653771],\n",
      "       [-1.21316959],\n",
      "       [ 0.18656703],\n",
      "       [ 0.17197271],\n",
      "       [ 0.69641682],\n",
      "       [ 0.31157991],\n",
      "       [-0.1237728 ],\n",
      "       [ 0.70581011],\n",
      "       [ 0.17540558],\n",
      "       [ 0.10929267]]), array([[2.70509797]])]\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('bank-note/train.csv', header=None)\n",
    "test_data = pd.read_csv('bank-note/test.csv', header=None)\n",
    "\n",
    "X_train = train_data.iloc[:, :-1].values\n",
    "y_train = train_data.iloc[:, -1].values\n",
    "X_test = test_data.iloc[:, :-1].values\n",
    "y_test = test_data.iloc[:, -1].values\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_sizes):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.weights = [np.random.randn(self.layer_sizes[i], self.layer_sizes[i-1]) for i in range(1, len(self.layer_sizes))]\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.layer_sizes[1:]]\n",
    "\n",
    "    def forward_pass(self, x):\n",
    "        activations = [x]\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, activations[-1]) + b\n",
    "            if w is not self.weights[-1]:\n",
    "                activations.append(sigmoid(z))\n",
    "            else:\n",
    "                activations.append(z)\n",
    "        return activations\n",
    "\n",
    "    def backpropagation(self, x, y):\n",
    "        activations = self.forward_pass(x)\n",
    "        weight_gradients = [np.zeros(w.shape) for w in self.weights]\n",
    "        bias_gradients = [np.zeros(b.shape) for b in self.biases]\n",
    "\n",
    "        delta = activations[-1] - y \n",
    "\n",
    "        weight_gradients[-1] = np.dot(delta, activations[-2].T)\n",
    "        bias_gradients[-1] = delta\n",
    "\n",
    "        for l in range(2, len(self.layer_sizes)):\n",
    "            z = np.dot(self.weights[-l+1].T, delta)\n",
    "            delta = z * sigmoid_derivative(activations[-l])\n",
    "            weight_gradients[-l] = np.dot(delta, activations[-l-1].T)\n",
    "            bias_gradients[-l] = delta\n",
    "\n",
    "        return weight_gradients, bias_gradients\n",
    "\n",
    "    def train(self, X, y, epochs, gamma_0, d):\n",
    "        for epoch in range(epochs):\n",
    "            permutation = np.random.permutation(X.shape[0])\n",
    "            X_shuffled = X[permutation]\n",
    "            y_shuffled = y[permutation]\n",
    "\n",
    "            for i, (x, y_true) in enumerate(zip(X_shuffled, y_shuffled)):\n",
    "                x = x.reshape(-1, 1)\n",
    "                y_true = np.array([[y_true]])\n",
    "\n",
    "                gamma_t = gamma_0 / (1 + (gamma_0 / d) * i)\n",
    "\n",
    "                weight_gradients, bias_gradients = self.backpropagation(x, y_true)\n",
    "                self.weights = [w - gamma_t * dw for w, dw in zip(self.weights, weight_gradients)]\n",
    "                self.biases = [b - gamma_t * db for b, db in zip(self.biases, bias_gradients)]\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            x = x.reshape(-1, 1)\n",
    "            activations = self.forward_pass(x)\n",
    "            predictions.append(activations[-1][0, 0] > 0.5)\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        accuracy = np.mean(predictions == y)\n",
    "        return accuracy\n",
    "    \n",
    "    #initialize weights to 0\n",
    "    def zero_init(self):\n",
    "        self.weights = [np.zeros((self.layer_sizes[i], self.layer_sizes[i-1])) for i in range(1, len(self.layer_sizes))]\n",
    "        self.biases = [np.zeros((y, 1)) for y in self.layer_sizes[1:]]\n",
    "        return self.weights, self.biases\n",
    "    \n",
    "print(\"Question 2A: Backpropagation\")\n",
    "nn = NeuralNetwork([4, 10, 10, 1])\n",
    "x_sample = X_train[0].reshape(-1, 1)\n",
    "y_sample = np.array([[y_train[0]]])\n",
    "weight_gradients, bias_gradients = nn.backpropagation(x_sample, y_sample)\n",
    "print(\"Backpropagation Gradients Example:\")\n",
    "print(\"Weight Gradients:\", weight_gradients)\n",
    "print(\"Bias Gradients:\", bias_gradients)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T21:13:33.701427Z",
     "start_time": "2023-12-10T21:13:33.691761Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 2B: Stochastic Gradient Descent\n",
      "Width: 5, Training Accuracy: 0.8692660550458715, Test Accuracy: 0.864\n",
      "Width: 10, Training Accuracy: 0.8646788990825688, Test Accuracy: 0.854\n",
      "Width: 25, Training Accuracy: 0.9094036697247706, Test Accuracy: 0.894\n",
      "Width: 50, Training Accuracy: 0.8107798165137615, Test Accuracy: 0.818\n",
      "Width: 100, Training Accuracy: 0.698394495412844, Test Accuracy: 0.678\n"
     ]
    }
   ],
   "source": [
    "print(\"Question 2B: Stochastic Gradient Descent\")\n",
    "widths = [5, 10, 25, 50, 100]\n",
    "for width in widths:\n",
    "    nn = NeuralNetwork([X_train.shape[1], width, width, 1])\n",
    "    nn.train(X_train, y_train, epochs=100, gamma_0=0.01, d=0.01)\n",
    "    training_accuracy = nn.evaluate(X_train, y_train)\n",
    "    test_accuracy = nn.evaluate(X_test, y_test)\n",
    "    print(f\"Width: {width}, Training Accuracy: {training_accuracy}, Test Accuracy: {test_accuracy}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T21:13:55.323140Z",
     "start_time": "2023-12-10T21:13:35.050473Z"
    }
   },
   "id": "dde8de0306c008d0"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Width: 5, Zero-Initialized Weights, Training Accuracy: 0.9094036697247706, Test Accuracy: 0.894\n",
      "Width: 10, Zero-Initialized Weights, Training Accuracy: 0.9174311926605505, Test Accuracy: 0.92\n",
      "Width: 25, Zero-Initialized Weights, Training Accuracy: 0.838302752293578, Test Accuracy: 0.828\n",
      "Width: 50, Zero-Initialized Weights, Training Accuracy: 0.8635321100917431, Test Accuracy: 0.878\n",
      "Width: 100, Zero-Initialized Weights, Training Accuracy: 0.768348623853211, Test Accuracy: 0.744\n"
     ]
    }
   ],
   "source": [
    "widths = [5, 10, 25, 50, 100]\n",
    "for width in widths:\n",
    "    nn_zero_init = NeuralNetwork([X_train.shape[1], width, width, 1])\n",
    "    nn_zero_init.train(X_train, y_train, epochs=100, gamma_0=0.01, d=0.01)  \n",
    "    training_accuracy_zero_init = nn_zero_init.evaluate(X_train, y_train)\n",
    "    test_accuracy_zero_init = nn_zero_init.evaluate(X_test, y_test)\n",
    "    print(f\"Width: {width}, Zero-Initialized Weights, Training Accuracy: {training_accuracy_zero_init}, Test Accuracy: {test_accuracy_zero_init}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T20:34:08.625598Z",
     "start_time": "2023-12-10T20:33:46.201936Z"
    }
   },
   "id": "80f7950713db3c3e"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth: 3, Width: 5, Activation: tanh, Train Accuracy: 99.54%, Test Accuracy: 99.40%\n",
      "Depth: 3, Width: 5, Activation: relu, Train Accuracy: 99.31%, Test Accuracy: 99.20%\n",
      "Depth: 3, Width: 10, Activation: tanh, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 3, Width: 10, Activation: relu, Train Accuracy: 99.77%, Test Accuracy: 100.00%\n",
      "Depth: 3, Width: 25, Activation: tanh, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 3, Width: 25, Activation: relu, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 3, Width: 50, Activation: tanh, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 3, Width: 50, Activation: relu, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 3, Width: 100, Activation: tanh, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 3, Width: 100, Activation: relu, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 5, Width: 5, Activation: tanh, Train Accuracy: 99.89%, Test Accuracy: 99.60%\n",
      "Depth: 5, Width: 5, Activation: relu, Train Accuracy: 99.66%, Test Accuracy: 99.80%\n",
      "Depth: 5, Width: 10, Activation: tanh, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 5, Width: 10, Activation: relu, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 5, Width: 25, Activation: tanh, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 5, Width: 25, Activation: relu, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 5, Width: 50, Activation: tanh, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 5, Width: 50, Activation: relu, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 5, Width: 100, Activation: tanh, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 5, Width: 100, Activation: relu, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 9, Width: 5, Activation: tanh, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 9, Width: 5, Activation: relu, Train Accuracy: 85.65%, Test Accuracy: 81.36%\n",
      "Depth: 9, Width: 10, Activation: tanh, Train Accuracy: 100.00%, Test Accuracy: 99.80%\n",
      "Depth: 9, Width: 10, Activation: relu, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 9, Width: 25, Activation: tanh, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 9, Width: 25, Activation: relu, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 9, Width: 50, Activation: tanh, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 9, Width: 50, Activation: relu, Train Accuracy: 100.00%, Test Accuracy: 99.80%\n",
      "Depth: 9, Width: 100, Activation: tanh, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 9, Width: 100, Activation: relu, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def create_dataloader(csv_file, batch_size=32):\n",
    "    data = pd.read_csv(csv_file)\n",
    "    features = torch.tensor(data.iloc[:, :-1].values, dtype=torch.float32)\n",
    "    targets = torch.tensor(data.iloc[:, -1].values, dtype=torch.int64)\n",
    "    dataset = TensorDataset(features, targets)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "train_loader = create_dataloader('bank-note/train.csv')\n",
    "test_loader = create_dataloader('bank-note/test.csv')\n",
    "\n",
    "\n",
    "class PTNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_layers, activation_fn):\n",
    "        super(PTNeuralNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_layers)):\n",
    "            if i == 0:\n",
    "                self.layers.append(nn.Linear(input_size, hidden_layers[i]))\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(hidden_layers[i - 1], hidden_layers[i]))\n",
    "        self.layers.append(nn.Linear(hidden_layers[-1], output_size))\n",
    "        self.activation_fn = activation_fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            if i < len(self.layers) - 1:\n",
    "                x = self.activation_fn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def evaluate_accuracy(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "\n",
    "def train_and_evaluate(depths, widths, activation_fns, init_methods, train_loader, test_loader):\n",
    "    input_size = 4\n",
    "    output_size = 2\n",
    "    num_epochs = 20\n",
    "\n",
    "    for depth in depths:\n",
    "        for width in widths:\n",
    "            for activation_fn, init_method in zip(activation_fns, init_methods):\n",
    "                hidden_layers = [width] * depth\n",
    "                model = PTNeuralNetwork(input_size, output_size, hidden_layers, activation_fn)\n",
    "                for layer in model.layers:\n",
    "                    if isinstance(layer, nn.Linear):\n",
    "                        init_method(layer.weight)\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "                for epoch in range(num_epochs):\n",
    "                    model.train()\n",
    "                    for data, target in train_loader:\n",
    "                        optimizer.zero_grad()\n",
    "                        output = model(data)\n",
    "                        loss = criterion(output, target)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                train_accuracy = evaluate_accuracy(model, train_loader)\n",
    "                test_accuracy = evaluate_accuracy(model, test_loader)\n",
    "                print(f\"Depth: {depth}, Width: {width}, Activation: {activation_fn.__name__}, \"\n",
    "                      f\"Train Accuracy: {train_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "activation_fns = [torch.tanh, torch.relu]\n",
    "init_methods = [nn.init.xavier_normal_, nn.init.kaiming_normal_]\n",
    "\n",
    "train_and_evaluate([3, 5, 9], [5, 10, 25, 50, 100], activation_fns, init_methods, train_loader, test_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T20:30:37.044948Z",
     "start_time": "2023-12-10T20:29:53.024890Z"
    }
   },
   "id": "a28fe48f2a63cf00"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
